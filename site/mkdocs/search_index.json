{
    "docs": [
        {
            "location": "/",
            "text": "Welcome\n\n\nWelcome to the ParaView for Geophysics Documentation! This repository is all about using ParaView in the geosciences for data and model visualization. Through visualization, we can bring value to data and hold the products of geoscience in a new light to interested parties.\n\n\nThis repository was produced from the work of an undergraduate research project at the Colorado School of Mines titled: Illuminating the Value of Geophysical Imaging through Visualization and Virtual Reality. Checkout \nthis PDF\n standalone presentation to learn more about the project.\n\n\nUse the Sidebar (to the left) to explore the docs page and to find all documentation for readers, filters, macros, and more as you need.\n\n\nDemo\n\n\nCheck out the data scene below. This is an example of three data sets visually integrated using our framework and exported to a shareable format. Go ahead, click it and move it around!\n\n\n\n        \n\n\n\n\n\n\n\nAbout the Project\n\n\nThe primary goal of this project is to build plugins for the open-source, multi-platform, data analysis, and visualization application \nParaView\n by Kitware. These plugins are tailored to the visualization of spatially referenced data in the geosciences. The overarching  goal of this project is to develop a framework to funnel geophysical data/models into virtual reality for the purpose of:\n\n\n\n\nExtracting Value of Information (VOI)\n\n\nUser/Stakeholder engagement with geophysical findings\n\n\nCommunicating uncertainty in an useable way\n\n\n\n\nMy specific goal is to develop a heavily documented library of plugins, macros, and examples of how to view standard formats of geoscientific and geophysical data on the ParaView software platform. These plugins will provide tools to perform post-processing visual analysis and interpretation of geoscientific data and models.\n\n\nThrough the deployment of this software, geophysicists will gain an ability to represent their 3D spatially referenced data intuitively to interested parties and stakeholders. By integrating the visualization of various data, interested parties will gain insight into the value of the information in the models. A spatially defined 3D model yields minimal value to an outside party unless they can relate that model to other spatial features. For example, a 3D model of faults in the subsurface is unhelpful unless the location of known features to interested parties can be displayed simultaneously. To give a value of information, we must be able to show where the spatially referenced data is in relation to intuitive features like topography, well locations, survey points, or other known features. Through visual integration, we try to mimic the reality of the space in which data was acquired so that it will hold meaning to anyone that immerses into the visualization regardless of background.\n\n\nRecent Abstract Submission\n\n\nThe results of geophysical imaging techniques often hold high significance to stakeholders in the problems addressed yet the effective perception of those results remains a dynamic challenge for all. To illuminate the value of geophysical imaging techniques, we are developing a framework to visually integrate geophysical data and models in 3D which extends into Virtual Reality (VR) as well as statistically analyzing interpretation advantages in VR. The motivation for this effort comes from a desire to directly engage stakeholders with geophysical data gaining Value of Information (VOI) and de-risking decision making in project planning. This framework is a code base that extends the functionality of the open-source visualization platform ParaView by Kitware. These extensions make it possible to visually integrate geophysical data in a multidimensional rendering space so that the end product is interpretable to non-geoscientists and that all parties can gain insight and VOI from geophysical imaging techniques. To show value in the VR presentation of multi-dimensional visualizations, we aim to develop metrics that will analyze the effectiveness of visual analysis in VR compared to traditional methods. We will evaluate these metrics through statistical gaming type protocol, where we will task subjects with making spatial decisions and finding features of interest in complex geoscientific scenes. We hypothesize that VR will bring the needed perception to most efficiently make spatial decisions and detect features of interest as well as convey information such as uncertainty in a usable manner. We will have preliminary results of the gaming protocol by March 2018 as well as share our visual framework along that journey in the form of a GitHub repository titled \u201cParaViewGeophysics.\u201d Our goal in sharing the repository is to deliver a toolset that enables geophysicists to rapidly visualize their data and models as well as effectively communicate their findings to interested stakeholders.\n\n\nOutline of Goals\n\n\nThese are the goals to achieve through publishing this repository but not necessarily the goals of the research project from which this repository was developed.\n\n\n\n\n\n\nDevelop and document geoscientific plugins for ParaView. These plugins will take advantage of ParaView and VTK\u2019s Python wrapping and use the Python Programmable Filter in ParaView. The advantage to using Python Programmable filters is that they are easily modified by the end user and can be wrapped in XML to create a GUI for its use in ParaView while having the option to directly edit the source code live in ParaView.\n\n\n\n\n\n\nReaders: Plugins that read common geoscientific and geophysical spatial data files (GSLIB, UBC mesh, ESRI grid, etc.). Also make readers that read common raw data files (packed binary floats, delimited ASCII, etc.)\n\n\n\n\n\n\nFilters: Plugins that perform post-processing analysis of geoscientific data for visualization. For example, filters that build tubes from a series of points that represent a tunnel or filters that take a 1D array, reshape it to 2D or 3D, and make a volumetric model ready for visualization all while adding spatial reference for visual integration.  \n\n\n\n\n\n\nSources: Plugins that create simple synthetic data sources such as a sphere or cube with a specified attribute like a spatially varying density or electrical conductivity. Other sources might include using that synthetic sphere or cube to make a volumetric field of some response.\n\n\n\n\n\n\n\n\n\n\nMake tutorials on the use of ParaView\u2019s native features and the plugins distributed in this repository on open source data (for example):\n\n\n\n\n\n\nDocument explanations of how to get sophisticated geoscientific data formats like topography DEMs into a format ParaView can read.\n\n\n\n\n\n\nDocument how to use ParaView\u2019s native filters to complete common tasks in the visualization of geoscientific data. For example, applying satellite imagery to a surface that represents topography.\n\n\n\n\n\n\n\n\n\n\nDevelop customizable macros for the visualization of common data formats. This will include developing macros on an individual basis to help others quickly visualize their data and models for quality assessment and individual research needs.\n\n\n\n\n\n\n\n\nAbout the Author\n\n\nUnless otherwise specified at the top of the file, all code and documentation distributed here were produced by \nBane Sullivan\n, undergraduate research assistant in the Geophysics Department at the Colorado School of Mines under Dr. Whitney J. Trainor-Guitton. Feel free to contact Bane for questions or for custom filters/readers to visualize geoscience data through the \nIssues page\n\n\n\n\nFeatures on Their Way\n\n\nHere is a list of features that are shortly coming to this repo. This list will be regularly updated\n\n\nDocumentation is soon to come. We want to do it right: with tutorials, examples, and detailed justification for need and use of each reader and filter.\n\n\nSuggestions? Post on the \nIssues page\n as a feature request.\n\n\nGeneral Features\n\n\n\n\n How to send data scenes made using the Readers, Filters, and Macros in this repository over to the Virtual Reality build of ParaView\n\n\n How to build your own plugins using this project\u2019s framework and build scripts\n\n\n\n\nReaders\n\n\n\n\n \nUBC Mesh:\n both 2D and 3D. Details \nhere\n and \nhere\n.\n\n\n \nESRI Grid:\n Details \nhere\n and \nhere\n\n\n \nESRI shape files:\n Details \nhere\n and \nhere\n\n\n \nWell logs:\n Readers for common formats and easy ways to project well logs in XYZ space\n\n\n\n\nFilters\n\n\n\n\n \nExtract Array:\n This will allow you to extract any array from any data structure as vtkPolyData.\n\n\n \nTranspose Grid:\n Transpose or swap axii of grid data sets (vtkImageData and vtkRectilinearGrid)\n\n\n \nReshape Table:\n Adding ability to reshape using fortran ordering on the currently available filter.\n\n\n \nMake Cubes from Point Set:\n This will take a point set and generate cube of some specified size at every point\n\n\n\n\n\n\n\nMacros\n\n\n\n\n How to start making your own macros (tips, tricks, and general advice)\n\n\n Save screenshots in isometric views, side, top, etc. views\n\n\n Coming to all macros: ability to use a file selection prompt instead of hardcoding file names into the scripts.\n\n\n \nMany Slices Along Points:\n Export slices of data set along poly line at every point on that line (normal is the vector from that point to the next)\n\n\n\n\nExamples\n\n\n\n\n Tutorials for each filter / reader will be in their documentation.\n\n\n How to export a scene to a shareable 3D format\n\n\n Importing DEM topography (with/without satellite imagery)\n\n\n Slicing/cropping a data scene through all components/datasets\n\n\n \nSlice Model Along PolyLine:\n How to export a slice of a data set projected on a vtkPolyLine (capabilities are currently present in ParaView)",
            "title": "Home"
        },
        {
            "location": "/#welcome",
            "text": "Welcome to the ParaView for Geophysics Documentation! This repository is all about using ParaView in the geosciences for data and model visualization. Through visualization, we can bring value to data and hold the products of geoscience in a new light to interested parties.  This repository was produced from the work of an undergraduate research project at the Colorado School of Mines titled: Illuminating the Value of Geophysical Imaging through Visualization and Virtual Reality. Checkout  this PDF  standalone presentation to learn more about the project.  Use the Sidebar (to the left) to explore the docs page and to find all documentation for readers, filters, macros, and more as you need.",
            "title": "Welcome"
        },
        {
            "location": "/#demo",
            "text": "Check out the data scene below. This is an example of three data sets visually integrated using our framework and exported to a shareable format. Go ahead, click it and move it around!",
            "title": "Demo"
        },
        {
            "location": "/#about-the-project",
            "text": "The primary goal of this project is to build plugins for the open-source, multi-platform, data analysis, and visualization application  ParaView  by Kitware. These plugins are tailored to the visualization of spatially referenced data in the geosciences. The overarching  goal of this project is to develop a framework to funnel geophysical data/models into virtual reality for the purpose of:   Extracting Value of Information (VOI)  User/Stakeholder engagement with geophysical findings  Communicating uncertainty in an useable way   My specific goal is to develop a heavily documented library of plugins, macros, and examples of how to view standard formats of geoscientific and geophysical data on the ParaView software platform. These plugins will provide tools to perform post-processing visual analysis and interpretation of geoscientific data and models.  Through the deployment of this software, geophysicists will gain an ability to represent their 3D spatially referenced data intuitively to interested parties and stakeholders. By integrating the visualization of various data, interested parties will gain insight into the value of the information in the models. A spatially defined 3D model yields minimal value to an outside party unless they can relate that model to other spatial features. For example, a 3D model of faults in the subsurface is unhelpful unless the location of known features to interested parties can be displayed simultaneously. To give a value of information, we must be able to show where the spatially referenced data is in relation to intuitive features like topography, well locations, survey points, or other known features. Through visual integration, we try to mimic the reality of the space in which data was acquired so that it will hold meaning to anyone that immerses into the visualization regardless of background.",
            "title": "About the Project"
        },
        {
            "location": "/#recent-abstract-submission",
            "text": "The results of geophysical imaging techniques often hold high significance to stakeholders in the problems addressed yet the effective perception of those results remains a dynamic challenge for all. To illuminate the value of geophysical imaging techniques, we are developing a framework to visually integrate geophysical data and models in 3D which extends into Virtual Reality (VR) as well as statistically analyzing interpretation advantages in VR. The motivation for this effort comes from a desire to directly engage stakeholders with geophysical data gaining Value of Information (VOI) and de-risking decision making in project planning. This framework is a code base that extends the functionality of the open-source visualization platform ParaView by Kitware. These extensions make it possible to visually integrate geophysical data in a multidimensional rendering space so that the end product is interpretable to non-geoscientists and that all parties can gain insight and VOI from geophysical imaging techniques. To show value in the VR presentation of multi-dimensional visualizations, we aim to develop metrics that will analyze the effectiveness of visual analysis in VR compared to traditional methods. We will evaluate these metrics through statistical gaming type protocol, where we will task subjects with making spatial decisions and finding features of interest in complex geoscientific scenes. We hypothesize that VR will bring the needed perception to most efficiently make spatial decisions and detect features of interest as well as convey information such as uncertainty in a usable manner. We will have preliminary results of the gaming protocol by March 2018 as well as share our visual framework along that journey in the form of a GitHub repository titled \u201cParaViewGeophysics.\u201d Our goal in sharing the repository is to deliver a toolset that enables geophysicists to rapidly visualize their data and models as well as effectively communicate their findings to interested stakeholders.",
            "title": "Recent Abstract Submission"
        },
        {
            "location": "/#outline-of-goals",
            "text": "These are the goals to achieve through publishing this repository but not necessarily the goals of the research project from which this repository was developed.    Develop and document geoscientific plugins for ParaView. These plugins will take advantage of ParaView and VTK\u2019s Python wrapping and use the Python Programmable Filter in ParaView. The advantage to using Python Programmable filters is that they are easily modified by the end user and can be wrapped in XML to create a GUI for its use in ParaView while having the option to directly edit the source code live in ParaView.    Readers: Plugins that read common geoscientific and geophysical spatial data files (GSLIB, UBC mesh, ESRI grid, etc.). Also make readers that read common raw data files (packed binary floats, delimited ASCII, etc.)    Filters: Plugins that perform post-processing analysis of geoscientific data for visualization. For example, filters that build tubes from a series of points that represent a tunnel or filters that take a 1D array, reshape it to 2D or 3D, and make a volumetric model ready for visualization all while adding spatial reference for visual integration.      Sources: Plugins that create simple synthetic data sources such as a sphere or cube with a specified attribute like a spatially varying density or electrical conductivity. Other sources might include using that synthetic sphere or cube to make a volumetric field of some response.      Make tutorials on the use of ParaView\u2019s native features and the plugins distributed in this repository on open source data (for example):    Document explanations of how to get sophisticated geoscientific data formats like topography DEMs into a format ParaView can read.    Document how to use ParaView\u2019s native filters to complete common tasks in the visualization of geoscientific data. For example, applying satellite imagery to a surface that represents topography.      Develop customizable macros for the visualization of common data formats. This will include developing macros on an individual basis to help others quickly visualize their data and models for quality assessment and individual research needs.",
            "title": "Outline of Goals"
        },
        {
            "location": "/#about-the-author",
            "text": "Unless otherwise specified at the top of the file, all code and documentation distributed here were produced by  Bane Sullivan , undergraduate research assistant in the Geophysics Department at the Colorado School of Mines under Dr. Whitney J. Trainor-Guitton. Feel free to contact Bane for questions or for custom filters/readers to visualize geoscience data through the  Issues page",
            "title": "About the Author"
        },
        {
            "location": "/#features-on-their-way",
            "text": "Here is a list of features that are shortly coming to this repo. This list will be regularly updated  Documentation is soon to come. We want to do it right: with tutorials, examples, and detailed justification for need and use of each reader and filter.  Suggestions? Post on the  Issues page  as a feature request.",
            "title": "Features on Their Way"
        },
        {
            "location": "/#general-features",
            "text": "How to send data scenes made using the Readers, Filters, and Macros in this repository over to the Virtual Reality build of ParaView   How to build your own plugins using this project\u2019s framework and build scripts",
            "title": "General Features"
        },
        {
            "location": "/#readers",
            "text": "UBC Mesh:  both 2D and 3D. Details  here  and  here .    ESRI Grid:  Details  here  and  here    ESRI shape files:  Details  here  and  here    Well logs:  Readers for common formats and easy ways to project well logs in XYZ space",
            "title": "Readers"
        },
        {
            "location": "/#filters",
            "text": "Extract Array:  This will allow you to extract any array from any data structure as vtkPolyData.    Transpose Grid:  Transpose or swap axii of grid data sets (vtkImageData and vtkRectilinearGrid)    Reshape Table:  Adding ability to reshape using fortran ordering on the currently available filter.    Make Cubes from Point Set:  This will take a point set and generate cube of some specified size at every point",
            "title": "Filters"
        },
        {
            "location": "/#macros",
            "text": "How to start making your own macros (tips, tricks, and general advice)   Save screenshots in isometric views, side, top, etc. views   Coming to all macros: ability to use a file selection prompt instead of hardcoding file names into the scripts.    Many Slices Along Points:  Export slices of data set along poly line at every point on that line (normal is the vector from that point to the next)",
            "title": "Macros"
        },
        {
            "location": "/#examples",
            "text": "Tutorials for each filter / reader will be in their documentation.   How to export a scene to a shareable 3D format   Importing DEM topography (with/without satellite imagery)   Slicing/cropping a data scene through all components/datasets    Slice Model Along PolyLine:  How to export a slice of a data set projected on a vtkPolyLine (capabilities are currently present in ParaView)",
            "title": "Examples"
        },
        {
            "location": "/Getting-Started/",
            "text": "A Brief Introduction to ParaView\n\n\nParaView is an open-source platform that can visualize 2D, 3D, and 4D (time varying) datasets. ParaView can process multiple very large data sets in parallel then later collect the results to yield a responsive graphics environment with which a user can interact. The better the processor and graphics hardware the machine or machines hosting the software, the faster and better ParaView will run, however it can run quite well on a laptop with a standard graphics card such as a MacBook Pro.\n\n\nSince ParaView is an open source application, anyone can download the program and its source code for modifications. The easiest way to get started with ParaView is to download the compiled binary installers for your operating system from \nhere\n.\n\n\nFor further help, check out the \ndocumentation\n provided by Kitware. In particular, the two worth looking through for a quick tour of ParaView are the \u2018The ParaView Guide\u2019 and \u2018The ParaView Tutorial.\u2019 One is a tutorial of the ParaView software and shows the user how to create sources, apply filters, and more. The other is a guide on how to do scripting, macros, and more intense use of the application.\n\n\nInstall ParaView\n\n\nOpen the downloaded binary installer and follow the prompts then drag the application into your applications folder.\n\n\nTour around software:\nTake a look at Section 2.1 of \u2018The ParaView Tutorial\u2019 for details of the application\u2019s GUI environment. Chapter 2 of the tutorial as a whole does an excellent job touring the software and its workflow for those unfamiliar with the software and its general capabilities.\n\n\nNotes\n\n\n\n\nEnable Auto Apply in the preferences for interactive slicers and so you don\u2019t always have to click apply. (\nNote:\n sometimes you may want this off when working with large datasets)\n\n\nOne convenient feature is to save the state of the ParaView environment. This saves all the options you selected on all the filters you applied to visualize some data. Select File->Save State\u2026 (\nNote:\n this saves the absolute path of the files loaded into ParaView, so be sure to select \u2018Search for Files Under Directory\u2026\u2019 when opening these state files)\n\n\n\n\n\n\nInstall ParaViewGeophysics\n\n\nTo clone and use the plugins distributed in the repo for ParaView, you\u2019ll need \nPython 2\n with the SciPy and NumPy modules \ninstalled\n, and \nParaView\n installed on your computer. Note that this repository will only work with builds of ParaView that have Python. Currently, the VR build of ParaView does not have Python included, and we will describe some workarounds for sending data to the VR version on under the Resources section in the \nDocs pages\n.\n\n\nWindows Users\n\n\nIf you\u2019re on Windows, see \nthis\n for GitHub and \nthis\n guide for using the Unix command line on windows.\n\n\nDownload and use \nCygwin\n for the command line operation of the scripts in this repo. When installing Cygwin, \nmake sure to install the \nbash\n, \ndos2unix\n, \ngit\n, and \npython2-setuptools\n packages\n. Now you can use the Cygwin terminal as the command line just like you are on a Unix based operating system! \nMake sure the line endings for all of the shell scripts are LF and not CRLF after cloning.\n\n\nAlso, be sure to place/install ParaView and this repository to a location that has general read/write privileges for all users such as on your \nD:\\\\\n drive. You will encounter all types of issues running the scripts and simply accessing the code via Cygwin if you need admin privileges to access where it is all saved. \nNote: the install scripts will need access to the directory where ParaView is installed\n\n\nBefore You Do Anything!\n\n\nYou \nMUST\n add a \nPVPATH\n variable in your bash profile! This variable will describe the path to ParaView\u2019s installation. It is likely different depending on your OS and your version of ParaView. On MacOS, simply just replace \n/ParaView-5.4.0.app\n with the name of your version of ParaView under \n/Applications/\n.\n\n\nAdd the \nPVPATH\n variable to your environment through your \n~/.bash_profile\n by adding this expression:\n\n\n# edit your ~/.bash_profile with vim or some text editor\n$ vi ~/.bash_profile\n\n# Be sure to check that this path matches yours... Odds are it's different!\n# Path to the ParaView installation:\nexport PVPATH=\"/Applications/ParaView-5.4.0.app\"\n\n\n\n\nWindows users, open Cygwin and edit your \n~/.bash_profile\n through \nvim\n to place a \nPVPATH\n variable in your environment. Be sure to replace the path to ParaView with your path to ParaView (e.g. \n/cygdrive/d/ParaView...\n to \n/cygdrive/d/ParaView-5.4.0...\n)\n\n\n## Be sure to check that this path matches yours... Odds are it's different!\n# Path to the ParaView installation:\nexport PVPATH=\"/cygdrive/d/ParaView\"\n\n\n\n\nRemember to source your edited \n~/.bash_profile\n:\n\n\n# Resource your profile to export the new variable\n$ source ~/.bash_profile\n\n\n\n\nCloning the Repository\n\n\nClone the repository from your command line by navigating to the directory you would like to save all of the code from this repo.\n\n\nNOTE:\n Windows users, you are going to want to clone to a folder/drive that has general read/write privileges such as your \nD:\\\\\n drive\n\n\nFrom your command line:\n\n\n# Clone this repository\n$ git clone https://github.com/banesullivan/ParaViewGeophysics\n\n# Go into the repository\n$ cd ParaViewGeophysics\n\n\n\n\nInstalling to ParaView\n\n\nNow to get started using the plugins and python modules included in this repository, we need to create links between your installation of ParaView and this repository. That\u2019s why we need a \nPVPATH\n variable to be set in your environment (above).\n\n\nTo create these links, run the installation script at the top of the repository called \ninstallMac.sh\n or \ninstallWin.sh\n for your operating system. If errors arise, they will be printed in red. The most common cause of error is having an incorrect \nPVPATH\n variable.\n\n\nNote: Windows users, when running the \ninstallWin.sh\n script there will be some yellow outputs. Make sure they are just announcing successful links and not errors/permission denied.\n\n\n# Install our repository to ParaView:\n#- Note: There are two install scripts. One for Mac/Linux and one for Windows\n#- Mac:\n$ sh ./installMac.sh\n#- Windows:\n$ sh ./installWin.sh\n\n\n\n\n\n\n\nHow to Update\n\n\nWe have included a script that will update the repository from GitHub and re-install everything. This script is simply executed by:\n\n\n$ sh ./updatePVGP.sh\n\n\n\n\n\n\nUsing Outside Modules\n\n\nParaView\u2019s Python environment, \npvpython\n, can be a bit tricky to start using outside Python modules like SciPy or SimPEG. On Mac OS X, using Python modules installed via pip or anaconda should work simply with an \nimport ...\n statement if you have your Python paths set up well. (Mac users: if you have trouble importing SciPy or other modules used in this repo let me know and I will develop a solution). Windows users on the other hand are going to have quite a bit of trouble as \npvpython\n is its own environment nestled in the ParaView application. I have not been able to develop an elegant solution for Windows users to use 3rd party Python libraries other than a simple copy/paste of that module into the ParaView application contents.\n\n\nNote: Advanced users, try a symbolic link between ParaView\u2019s python library and the libraries you have installed. This is how we are installing our module so that it stays updated.\n\n\nWindows Users\n\n\nTo start using third party libraries, we are going to have copy over static versions of the modules into ParaView\u2019s \nsite-packages\n directory. This folder should be under \n.../ParaView/bin/site-packages/\n. Effectively, we just perform a brute install of that module to \npvpython\n on Windows so that when we make \nimport\ns, the modules will be found in the \npvpython\n environment.\n\n\nModules Currently Needed for this Repo\n\n\nThese are all of the modules that filters, readers, and macros might use in this repo. We recommend opening the Python Shell from ParaView (Tools->Python Shell) and testing the import of each of these modules. Copy/Paste the modules that failed to import from wherever you have them installed into \n.../ParaView/bin/site-packages/\n:\n\n\n\n\nNumPy\n (you may need update ParaView\u2019s version to the latest version for SciPy to be happy)\n\n\nSciPy\n\n\nVTK\n\n\ndatetime\n\n\nstruct\n\n\ncsv\n\n\n\n\nI suspect there is a more elegant solution to setting up the Python environment for \npvpython\n on Windows to use modules installed from pip or anaconda, however Windows is not my area of expertise and I have had much trouble attempting to figure this out. I encourage a Windows user of this repository to figure this out so we don\u2019t have to make static version of these modules to use in ParaView.\n\n\nSomeone \nplease\n figure this out and post a more robust solution to the \nIssues page\n.\n\n\n\n\nDocumentation\n\n\nAll documentation for the code produced from this project will be included in this website. The documentation will contain an explanation of all of the produced plugins (filters and readers) and macros. Use the Sidebar to explore the documentation content and to find all documentation for readers, filters, macros, and more as you need. There are also details on how to \nbuild your own plugins\n, how to \nexport data scenes\n, and transferring your complex data scenes into \nvirtual reality\n.\n\n\nThe purpose to including all this extra documentation is to provide a convenient location for geoscientists to learn how to tailor ParaView to their needs because data representation and communication are an integral part of success in science. To effectively represent our spatial data is the first step to becoming successful and effective geoscientists. This is the principle behind why we are publishing this documentation along with the code in the repository. Not only do we want to effectively communicate the effort and motivation for this project, but we want to empower others to effectively communicate their scientific endeavors through spatial visualizations.\n\n\nPlugin Documentation\n\n\nThere is a page dedicated to every plugin in the respective readers and filters categories. On these pages, you will find implementation details, parameters, code quirks, and general usage information. As the project continues, we will also try to have an example for every reader and filter so that users can really get a feel for what is going on and how they might apply these plugins to address their needs. Since almost all geoscientific data is proprietary, these tutorials will likely come late so that we can find good open data sets and models that users can find outside of this repo for free.\n\n\nMacro documentation\n\n\n\nEach macro produced for this repository will have a distinct purpose, be it to export isometric screenshots of any data scene or to batch load / convert specific data sets. The macros will have broad applications and be formatted to work with generally any data scene or data of specific formats so that they can be easily expanded upon to complete specific tasks. For the macros we will try to immediately have sample data and a tutorial upon publishing with documentation of what we are doing and why. The Python scripts themselves will be heavily commented, so that every user can tailor the scripts to their individual needs.\n\n\nThere is also a detailed page on how to easily start making your own macros/scripts. From the people that I have talked to, there tends to be quite a steep learning curve with ParaView scripting, so we want to compile our knowledge into one place for others to gain tips, tricks, and advice to start making their own macros.\n\n\nRemember, if you have an idea for a macro, plugin, or would like to see how we would address a geoscientific visualization problem with ParaView, please post your thoughts on the \nissues page\n.",
            "title": "Getting Started"
        },
        {
            "location": "/Getting-Started/#a-brief-introduction-to-paraview",
            "text": "ParaView is an open-source platform that can visualize 2D, 3D, and 4D (time varying) datasets. ParaView can process multiple very large data sets in parallel then later collect the results to yield a responsive graphics environment with which a user can interact. The better the processor and graphics hardware the machine or machines hosting the software, the faster and better ParaView will run, however it can run quite well on a laptop with a standard graphics card such as a MacBook Pro.  Since ParaView is an open source application, anyone can download the program and its source code for modifications. The easiest way to get started with ParaView is to download the compiled binary installers for your operating system from  here .  For further help, check out the  documentation  provided by Kitware. In particular, the two worth looking through for a quick tour of ParaView are the \u2018The ParaView Guide\u2019 and \u2018The ParaView Tutorial.\u2019 One is a tutorial of the ParaView software and shows the user how to create sources, apply filters, and more. The other is a guide on how to do scripting, macros, and more intense use of the application.",
            "title": "A Brief Introduction to ParaView"
        },
        {
            "location": "/Getting-Started/#install-paraview",
            "text": "Open the downloaded binary installer and follow the prompts then drag the application into your applications folder.  Tour around software:\nTake a look at Section 2.1 of \u2018The ParaView Tutorial\u2019 for details of the application\u2019s GUI environment. Chapter 2 of the tutorial as a whole does an excellent job touring the software and its workflow for those unfamiliar with the software and its general capabilities.",
            "title": "Install ParaView"
        },
        {
            "location": "/Getting-Started/#notes",
            "text": "Enable Auto Apply in the preferences for interactive slicers and so you don\u2019t always have to click apply. ( Note:  sometimes you may want this off when working with large datasets)  One convenient feature is to save the state of the ParaView environment. This saves all the options you selected on all the filters you applied to visualize some data. Select File->Save State\u2026 ( Note:  this saves the absolute path of the files loaded into ParaView, so be sure to select \u2018Search for Files Under Directory\u2026\u2019 when opening these state files)",
            "title": "Notes"
        },
        {
            "location": "/Getting-Started/#install-paraviewgeophysics",
            "text": "To clone and use the plugins distributed in the repo for ParaView, you\u2019ll need  Python 2  with the SciPy and NumPy modules  installed , and  ParaView  installed on your computer. Note that this repository will only work with builds of ParaView that have Python. Currently, the VR build of ParaView does not have Python included, and we will describe some workarounds for sending data to the VR version on under the Resources section in the  Docs pages .",
            "title": "Install ParaViewGeophysics"
        },
        {
            "location": "/Getting-Started/#windows-users",
            "text": "If you\u2019re on Windows, see  this  for GitHub and  this  guide for using the Unix command line on windows.  Download and use  Cygwin  for the command line operation of the scripts in this repo. When installing Cygwin,  make sure to install the  bash ,  dos2unix ,  git , and  python2-setuptools  packages . Now you can use the Cygwin terminal as the command line just like you are on a Unix based operating system!  Make sure the line endings for all of the shell scripts are LF and not CRLF after cloning.  Also, be sure to place/install ParaView and this repository to a location that has general read/write privileges for all users such as on your  D:\\\\  drive. You will encounter all types of issues running the scripts and simply accessing the code via Cygwin if you need admin privileges to access where it is all saved.  Note: the install scripts will need access to the directory where ParaView is installed",
            "title": "Windows Users"
        },
        {
            "location": "/Getting-Started/#before-you-do-anything",
            "text": "You  MUST  add a  PVPATH  variable in your bash profile! This variable will describe the path to ParaView\u2019s installation. It is likely different depending on your OS and your version of ParaView. On MacOS, simply just replace  /ParaView-5.4.0.app  with the name of your version of ParaView under  /Applications/ .  Add the  PVPATH  variable to your environment through your  ~/.bash_profile  by adding this expression:  # edit your ~/.bash_profile with vim or some text editor\n$ vi ~/.bash_profile\n\n# Be sure to check that this path matches yours... Odds are it's different!\n# Path to the ParaView installation:\nexport PVPATH=\"/Applications/ParaView-5.4.0.app\"  Windows users, open Cygwin and edit your  ~/.bash_profile  through  vim  to place a  PVPATH  variable in your environment. Be sure to replace the path to ParaView with your path to ParaView (e.g.  /cygdrive/d/ParaView...  to  /cygdrive/d/ParaView-5.4.0... )  ## Be sure to check that this path matches yours... Odds are it's different!\n# Path to the ParaView installation:\nexport PVPATH=\"/cygdrive/d/ParaView\"  Remember to source your edited  ~/.bash_profile :  # Resource your profile to export the new variable\n$ source ~/.bash_profile",
            "title": "Before You Do Anything!"
        },
        {
            "location": "/Getting-Started/#cloning-the-repository",
            "text": "Clone the repository from your command line by navigating to the directory you would like to save all of the code from this repo.  NOTE:  Windows users, you are going to want to clone to a folder/drive that has general read/write privileges such as your  D:\\\\  drive  From your command line:  # Clone this repository\n$ git clone https://github.com/banesullivan/ParaViewGeophysics\n\n# Go into the repository\n$ cd ParaViewGeophysics",
            "title": "Cloning the Repository"
        },
        {
            "location": "/Getting-Started/#installing-to-paraview",
            "text": "Now to get started using the plugins and python modules included in this repository, we need to create links between your installation of ParaView and this repository. That\u2019s why we need a  PVPATH  variable to be set in your environment (above).  To create these links, run the installation script at the top of the repository called  installMac.sh  or  installWin.sh  for your operating system. If errors arise, they will be printed in red. The most common cause of error is having an incorrect  PVPATH  variable.  Note: Windows users, when running the  installWin.sh  script there will be some yellow outputs. Make sure they are just announcing successful links and not errors/permission denied.  # Install our repository to ParaView:\n#- Note: There are two install scripts. One for Mac/Linux and one for Windows\n#- Mac:\n$ sh ./installMac.sh\n#- Windows:\n$ sh ./installWin.sh",
            "title": "Installing to ParaView"
        },
        {
            "location": "/Getting-Started/#how-to-update",
            "text": "We have included a script that will update the repository from GitHub and re-install everything. This script is simply executed by:  $ sh ./updatePVGP.sh",
            "title": "How to Update"
        },
        {
            "location": "/Getting-Started/#using-outside-modules",
            "text": "ParaView\u2019s Python environment,  pvpython , can be a bit tricky to start using outside Python modules like SciPy or SimPEG. On Mac OS X, using Python modules installed via pip or anaconda should work simply with an  import ...  statement if you have your Python paths set up well. (Mac users: if you have trouble importing SciPy or other modules used in this repo let me know and I will develop a solution). Windows users on the other hand are going to have quite a bit of trouble as  pvpython  is its own environment nestled in the ParaView application. I have not been able to develop an elegant solution for Windows users to use 3rd party Python libraries other than a simple copy/paste of that module into the ParaView application contents.  Note: Advanced users, try a symbolic link between ParaView\u2019s python library and the libraries you have installed. This is how we are installing our module so that it stays updated.",
            "title": "Using Outside Modules"
        },
        {
            "location": "/Getting-Started/#windows-users_1",
            "text": "To start using third party libraries, we are going to have copy over static versions of the modules into ParaView\u2019s  site-packages  directory. This folder should be under  .../ParaView/bin/site-packages/ . Effectively, we just perform a brute install of that module to  pvpython  on Windows so that when we make  import s, the modules will be found in the  pvpython  environment.",
            "title": "Windows Users"
        },
        {
            "location": "/Getting-Started/#modules-currently-needed-for-this-repo",
            "text": "These are all of the modules that filters, readers, and macros might use in this repo. We recommend opening the Python Shell from ParaView (Tools->Python Shell) and testing the import of each of these modules. Copy/Paste the modules that failed to import from wherever you have them installed into  .../ParaView/bin/site-packages/ :   NumPy  (you may need update ParaView\u2019s version to the latest version for SciPy to be happy)  SciPy  VTK  datetime  struct  csv   I suspect there is a more elegant solution to setting up the Python environment for  pvpython  on Windows to use modules installed from pip or anaconda, however Windows is not my area of expertise and I have had much trouble attempting to figure this out. I encourage a Windows user of this repository to figure this out so we don\u2019t have to make static version of these modules to use in ParaView.  Someone  please  figure this out and post a more robust solution to the  Issues page .",
            "title": "Modules Currently Needed for this Repo"
        },
        {
            "location": "/Getting-Started/#documentation",
            "text": "All documentation for the code produced from this project will be included in this website. The documentation will contain an explanation of all of the produced plugins (filters and readers) and macros. Use the Sidebar to explore the documentation content and to find all documentation for readers, filters, macros, and more as you need. There are also details on how to  build your own plugins , how to  export data scenes , and transferring your complex data scenes into  virtual reality .  The purpose to including all this extra documentation is to provide a convenient location for geoscientists to learn how to tailor ParaView to their needs because data representation and communication are an integral part of success in science. To effectively represent our spatial data is the first step to becoming successful and effective geoscientists. This is the principle behind why we are publishing this documentation along with the code in the repository. Not only do we want to effectively communicate the effort and motivation for this project, but we want to empower others to effectively communicate their scientific endeavors through spatial visualizations.",
            "title": "Documentation"
        },
        {
            "location": "/Getting-Started/#plugin-documentation",
            "text": "There is a page dedicated to every plugin in the respective readers and filters categories. On these pages, you will find implementation details, parameters, code quirks, and general usage information. As the project continues, we will also try to have an example for every reader and filter so that users can really get a feel for what is going on and how they might apply these plugins to address their needs. Since almost all geoscientific data is proprietary, these tutorials will likely come late so that we can find good open data sets and models that users can find outside of this repo for free.",
            "title": "Plugin Documentation"
        },
        {
            "location": "/Getting-Started/#macro-documentation",
            "text": "Each macro produced for this repository will have a distinct purpose, be it to export isometric screenshots of any data scene or to batch load / convert specific data sets. The macros will have broad applications and be formatted to work with generally any data scene or data of specific formats so that they can be easily expanded upon to complete specific tasks. For the macros we will try to immediately have sample data and a tutorial upon publishing with documentation of what we are doing and why. The Python scripts themselves will be heavily commented, so that every user can tailor the scripts to their individual needs.  There is also a detailed page on how to easily start making your own macros/scripts. From the people that I have talked to, there tends to be quite a steep learning curve with ParaView scripting, so we want to compile our knowledge into one place for others to gain tips, tricks, and advice to start making their own macros.  Remember, if you have an idea for a macro, plugin, or would like to see how we would address a geoscientific visualization problem with ParaView, please post your thoughts on the  issues page .",
            "title": "Macro documentation"
        },
        {
            "location": "/Plugins/Plugins/",
            "text": "The Argument for Using Python Programmable Filters\n\n\nThe development of plugins for the ParaView software platform can seem like a daunting task at first. Creating CMakeLists, writing in C++ again for the first time in years, learning XML to create interactive GUI components, and integrating the plugins into the ParaView build is a major turnoff. To get around all that, we can use something Kitware has put into ParaView for the rapid development of plugins, Python Programmable Filters and Sources! Python is an incredibly easy language to learn and most if not all geoscientists have experience working in Python. In this repo, we aim to produce all plugins in the Python Programmable Filters and Sources format for the following reasons:\n\n\n\n\nRapid development: Through the templates, shell scripts, and XML converters provided in this repo, it is easy to prototype and develop a plugin for you needs in a matter of minutes\n\n\nComputational power: VTK has NumPy wrapping to allow for the use of Pythons complex numerical analysis libraries like SciPy and NumPy.\n\n\nEasy customization by end user: since most scientists know and use Python, they can easily dive into the source code delivered in this repo to tailor it to their needs\n\n\nLive source code editing in the ParaView program: toggle the advanced button on the properties panel and the Python source code for that filter pops up in real time for editing on the pipeline.\n\n\nEasy GUI component creation through the build scripting included in this repo\n\n\n\n\nReaders\n\n\nA reader takes data from files and puts them into the proper VTK data structures so that we can make conversions and visualize on the ParaView pipeline. ParaView comes native with a plethora of data format readers but there are a million more formats out there in the world. So by creating formats for common geoscientific formats, we hope to make the process of getting data into the ParaView pipeline as simple as possible. We also are doing this to show how easy it is to make custom readers and convey the message that if you can dream it, we can code it! Whatever your data format, there is likely a VTK data structure perfect for it.\n\n\nFilters\n\n\nA filter modifies, transforms, combines, analyses, etc. data on the ParaView pipeline. Filters provide a means for changing how we visualize data or create a means of taking some data and generating other data from that. For example we might have a series of scattered points that we know represent the center of a tunnel. We can use a filter to transform those points into a connected line that we then construct a tunnel around. This allows us to save out minimal data (just XYZ points as opposed to complex geometries that make up the tunnel) to our hard drive while still having complex visualizations from that data.",
            "title": "About ParaView Plugins"
        },
        {
            "location": "/Plugins/Plugins/#the-argument-for-using-python-programmable-filters",
            "text": "The development of plugins for the ParaView software platform can seem like a daunting task at first. Creating CMakeLists, writing in C++ again for the first time in years, learning XML to create interactive GUI components, and integrating the plugins into the ParaView build is a major turnoff. To get around all that, we can use something Kitware has put into ParaView for the rapid development of plugins, Python Programmable Filters and Sources! Python is an incredibly easy language to learn and most if not all geoscientists have experience working in Python. In this repo, we aim to produce all plugins in the Python Programmable Filters and Sources format for the following reasons:   Rapid development: Through the templates, shell scripts, and XML converters provided in this repo, it is easy to prototype and develop a plugin for you needs in a matter of minutes  Computational power: VTK has NumPy wrapping to allow for the use of Pythons complex numerical analysis libraries like SciPy and NumPy.  Easy customization by end user: since most scientists know and use Python, they can easily dive into the source code delivered in this repo to tailor it to their needs  Live source code editing in the ParaView program: toggle the advanced button on the properties panel and the Python source code for that filter pops up in real time for editing on the pipeline.  Easy GUI component creation through the build scripting included in this repo",
            "title": "The Argument for Using Python Programmable Filters"
        },
        {
            "location": "/Plugins/Plugins/#readers",
            "text": "A reader takes data from files and puts them into the proper VTK data structures so that we can make conversions and visualize on the ParaView pipeline. ParaView comes native with a plethora of data format readers but there are a million more formats out there in the world. So by creating formats for common geoscientific formats, we hope to make the process of getting data into the ParaView pipeline as simple as possible. We also are doing this to show how easy it is to make custom readers and convey the message that if you can dream it, we can code it! Whatever your data format, there is likely a VTK data structure perfect for it.",
            "title": "Readers"
        },
        {
            "location": "/Plugins/Plugins/#filters",
            "text": "A filter modifies, transforms, combines, analyses, etc. data on the ParaView pipeline. Filters provide a means for changing how we visualize data or create a means of taking some data and generating other data from that. For example we might have a series of scattered points that we know represent the center of a tunnel. We can use a filter to transform those points into a connected line that we then construct a tunnel around. This allows us to save out minimal data (just XYZ points as opposed to complex geometries that make up the tunnel) to our hard drive while still having complex visualizations from that data.",
            "title": "Filters"
        },
        {
            "location": "/Plugins/Build-Your-Own-Plugins/",
            "text": "Where to start\n\n\nDescription to come! There are a lot of pages in the documentation and we are trying to fill all content as soon as possible. Stay tuned for updates to this page",
            "title": "Build Your Own Plugins"
        },
        {
            "location": "/Plugins/Build-Your-Own-Plugins/#where-to-start",
            "text": "Description to come! There are a lot of pages in the documentation and we are trying to fill all content as soon as possible. Stay tuned for updates to this page",
            "title": "Where to start"
        },
        {
            "location": "/Plugins/Readers/Read-SGeMS-Grid/",
            "text": "About this Reader\n\n\nThe Stanford Geostatistical Modeling Software (SGeMS) ASCII format is much like the \nGSLIB\n file format. The reader we have developed for this format assumes the data to be defined on a regularly spaced grid and that the first line of the file will specify the dimensions of that grid. The output of this file reader is a vtkImageData object which is essentially a regularly spaced grid with varying dimensionality along each axis. The reader will only work if the format of the file strictly follows what is below. If your SGeMS file does not strictly follow the uniform grid format below then we recommend use the \nGSLIB\n file reader.\n\n\nFile Format\n\n\nThe general format is as follows:\n\n\nn1 n2 n3\nnumberOfColumns\nCol1_name\nCol2_name\nCol3_name\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\n...\n\n\n\n\nAn example file might look something like this, where we have a 400 by 150 by 40 (x by y by z) grid with uniform spacing along each axis with three data arrays:\n\n\n400 150 40\n3\nVariable1\nVariable2\nVariable3\n0.908793985844 -0.141859993339 0.76693302393\n0.909209012985 0.0264630001038 0.935671985149\n0.908389985561 -0.0224980004132 0.885891973972\n0.906355023384 -0.0762720033526 0.83008402586\n0.895779013634 0.0125150000677 0.908294022083\n0.876645028591 -0.0550080016255 0.821636974812\n0.856096029282 0.0719339996576 0.928031027317\n...\n\n\n\n\nDown the Pipeline\n\n\n\n\nTranslate Origin of Grid\n\n\nFlip Grid Axii\n\n\nNormalize Array\n\n\nContour\n\n\nThreshold\n\n\n\n\nExample Use\n\n\nFor example files to use with this reader, download any of the 2D or 3D files from \nthis website\n and load them into ParaView using the \u2018Read SGeMS File to Uniform Grid\u2019 file reader. A 2D or 3D block of data should automatically be built and visualized.\n\n\nHere is the \nWalker Lake Exhaustive DEM Categorized\n with a categorized color scale:\n\n\n\n\nand here is the \nFLUVSIM object-based model\n with a categorized color scale (bounding surfaces are set to be transparent):\n\n\n\n\nWe will later add in the ability to specify the spacing and origin of the produced vtkImageData as advanced properties of this reader, however you can easily do this by adding a Python Programmable Filter that copies the data and changes these properties with a script like this one:\n\n\n# Note: Set the output type to vtkImageData\npdi = self.GetInput() # vtkImageData\npdo = self.GetOutput() #vtkImageData\n# DeepCopy so that we do not disturb the input data\npdo.DeepCopy(pdi)\n# x, y, z origin\npdo.SetOrigin(x_origin, y_origin, z_origin)\n# spacing for each axial direction\npdo.SetSpacing(x_spacing, y_spacing, z_spacing)",
            "title": "Read SGeMS Grid"
        },
        {
            "location": "/Plugins/Readers/Read-SGeMS-Grid/#about-this-reader",
            "text": "The Stanford Geostatistical Modeling Software (SGeMS) ASCII format is much like the  GSLIB  file format. The reader we have developed for this format assumes the data to be defined on a regularly spaced grid and that the first line of the file will specify the dimensions of that grid. The output of this file reader is a vtkImageData object which is essentially a regularly spaced grid with varying dimensionality along each axis. The reader will only work if the format of the file strictly follows what is below. If your SGeMS file does not strictly follow the uniform grid format below then we recommend use the  GSLIB  file reader.",
            "title": "About this Reader"
        },
        {
            "location": "/Plugins/Readers/Read-SGeMS-Grid/#file-format",
            "text": "The general format is as follows:  n1 n2 n3\nnumberOfColumns\nCol1_name\nCol2_name\nCol3_name\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\n...  An example file might look something like this, where we have a 400 by 150 by 40 (x by y by z) grid with uniform spacing along each axis with three data arrays:  400 150 40\n3\nVariable1\nVariable2\nVariable3\n0.908793985844 -0.141859993339 0.76693302393\n0.909209012985 0.0264630001038 0.935671985149\n0.908389985561 -0.0224980004132 0.885891973972\n0.906355023384 -0.0762720033526 0.83008402586\n0.895779013634 0.0125150000677 0.908294022083\n0.876645028591 -0.0550080016255 0.821636974812\n0.856096029282 0.0719339996576 0.928031027317\n...",
            "title": "File Format"
        },
        {
            "location": "/Plugins/Readers/Read-SGeMS-Grid/#down-the-pipeline",
            "text": "Translate Origin of Grid  Flip Grid Axii  Normalize Array  Contour  Threshold",
            "title": "Down the Pipeline"
        },
        {
            "location": "/Plugins/Readers/Read-SGeMS-Grid/#example-use",
            "text": "For example files to use with this reader, download any of the 2D or 3D files from  this website  and load them into ParaView using the \u2018Read SGeMS File to Uniform Grid\u2019 file reader. A 2D or 3D block of data should automatically be built and visualized.  Here is the  Walker Lake Exhaustive DEM Categorized  with a categorized color scale:   and here is the  FLUVSIM object-based model  with a categorized color scale (bounding surfaces are set to be transparent):   We will later add in the ability to specify the spacing and origin of the produced vtkImageData as advanced properties of this reader, however you can easily do this by adding a Python Programmable Filter that copies the data and changes these properties with a script like this one:  # Note: Set the output type to vtkImageData\npdi = self.GetInput() # vtkImageData\npdo = self.GetOutput() #vtkImageData\n# DeepCopy so that we do not disturb the input data\npdo.DeepCopy(pdi)\n# x, y, z origin\npdo.SetOrigin(x_origin, y_origin, z_origin)\n# spacing for each axial direction\npdo.SetSpacing(x_spacing, y_spacing, z_spacing)",
            "title": "Example Use"
        },
        {
            "location": "/Plugins/Readers/Read-GSLIB-File-to-Table/",
            "text": "About this Reader\n\n\nThe GSLIB file format has headers lines followed by the data as a space delimited ASCI file (this filter is set up to allow you to choose any single character delimiter: default is a space). The first header line contains the title or necessary information and will be printed to the console. This line may have the dimensions for a grid to be made of the data. The second line is the number (n) of columns of data. The next n lines are the variable names for the data in each column. You are allowed up to ten characters for the variable name. The data follow with a space between each field (column). The output of this reader is a vtkTable of the input data. The table will have all the same columns as the input file with the column/data names set to their respective names from the input file.\n\n\nFile Format\n\n\nCheck out \nthis site\n and \nthis site\n for more information on the specifics of the file format. The general format is as follows:\n\n\nHeader\nnumberOfColumns\nCol1_name\nCol2_name\nCol3_name\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\n...\n\n\n\n\nAn example file might look something like this:\n\n\nFun data set!\n3\nVariable1\nVariable2\nVariable3\n0.908793985844 -0.141859993339 0.76693302393\n0.909209012985 0.0264630001038 0.935671985149\n0.908389985561 -0.0224980004132 0.885891973972\n0.906355023384 -0.0762720033526 0.83008402586\n0.895779013634 0.0125150000677 0.908294022083\n0.876645028591 -0.0550080016255 0.821636974812\n0.856096029282 0.0719339996576 0.928031027317\n...\n\n\n\n\nDown the Pipeline\n\n\n\n\nTable to Uniform Grid\n\n\nReshape Table\n\n\nTable to Points\n\n\nTable to Structured Grid\n\n\nNormalize Array",
            "title": "Read GSLIB File"
        },
        {
            "location": "/Plugins/Readers/Read-GSLIB-File-to-Table/#about-this-reader",
            "text": "The GSLIB file format has headers lines followed by the data as a space delimited ASCI file (this filter is set up to allow you to choose any single character delimiter: default is a space). The first header line contains the title or necessary information and will be printed to the console. This line may have the dimensions for a grid to be made of the data. The second line is the number (n) of columns of data. The next n lines are the variable names for the data in each column. You are allowed up to ten characters for the variable name. The data follow with a space between each field (column). The output of this reader is a vtkTable of the input data. The table will have all the same columns as the input file with the column/data names set to their respective names from the input file.",
            "title": "About this Reader"
        },
        {
            "location": "/Plugins/Readers/Read-GSLIB-File-to-Table/#file-format",
            "text": "Check out  this site  and  this site  for more information on the specifics of the file format. The general format is as follows:  Header\nnumberOfColumns\nCol1_name\nCol2_name\nCol3_name\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\ndataCol1 dataCol2 dataCol3\n...  An example file might look something like this:  Fun data set!\n3\nVariable1\nVariable2\nVariable3\n0.908793985844 -0.141859993339 0.76693302393\n0.909209012985 0.0264630001038 0.935671985149\n0.908389985561 -0.0224980004132 0.885891973972\n0.906355023384 -0.0762720033526 0.83008402586\n0.895779013634 0.0125150000677 0.908294022083\n0.876645028591 -0.0550080016255 0.821636974812\n0.856096029282 0.0719339996576 0.928031027317\n...",
            "title": "File Format"
        },
        {
            "location": "/Plugins/Readers/Read-GSLIB-File-to-Table/#down-the-pipeline",
            "text": "Table to Uniform Grid  Reshape Table  Table to Points  Table to Structured Grid  Normalize Array",
            "title": "Down the Pipeline"
        },
        {
            "location": "/Plugins/Readers/Read-UBC-Mesh-Two-File-Format/",
            "text": "More to come!\n\n\n\n\n\nUBC Mesh 3D models are defined using a 2-file format. The \u201cmesh\u201d file describes how the data is descritized. The \u201cmodel\u201d file lists the physical property values for all cells in a mesh. A model file is meaningless without an associated mesh file. Default file delimiter is a space character.\n\n\nCurrently 3D mesh files work. We are implementing 2D compatibility currently.",
            "title": "Read UBC Mesh Two File FormatRe"
        },
        {
            "location": "/Plugins/Readers/Read-Binary-Packed-Data-to-Table/",
            "text": "About this Reader\n\n\nThis filter reads in float or double data that is packed into a binary file format. It will treat the data as one long array and make a vtkTable with one column of that data. The reader uses big endian and defaults to import as floats. Use the \u2018Table to Uniform Grid\u2019 or the \u2018Reshape Table\u2019 filters to give more meaning to the data. We chose to use a vtkTable object as the output of this reader because it gives us more flexibility in the filters we can apply to this data down the pipeline and keeps thing simple when using filters in this repository.\n\n\nThere is a checkbox you can use to specify double precision if needed and a text box where you can specify the data variable name. The default for the data variable name is the base name of the file, however you may want something less verbose or more specific.\n\n\nDown the Pipeline\n\n\n\n\nTable to Uniform Grid\n\n\nReshape Table\n\n\nTable to Points\n\n\nTable to Structured Grid\n\n\nNormalize Array\n\n\n\n\nExample Use\n\n\nFor an example of how to use this reader, lets make our own test file of packed floats in binary format. Run this script outside of ParaView to create a test file. It will compute a ripple function and write out all the \nZ\n data as packed floats.\n\n\nimport struct\nimport numpy as np\n\n# The 2D space we are working with\nstep = np.pi/64.\nx = y = np.arange(-np.pi,np.pi + step, step)\nX, Y = np.meshgrid(x,y)\n\n# A cool looking ripple function\ndef f(x,y):\n    return np.sin((x**2 + y**2))\n\n# Compute the values of the ripple function in our space\nZ = np.zeros(np.shape(X), dtype=np.float32)\nfor i in range(len(x)):\n    for j in range(len(y)):\n        Z[i,j] = f(X[i,j],Y[i,j])\n\n# Parameters for a 'Table to Uniform Grid' filter\nprint(\"Shape of grid (n1,n2,n3): \", np.shape(Z))\nprint(\"Spacing for all axii: \", step)\nprint(\"Origin (x,y,z): \", (np.min(x),np.min(y),0.0))\n\n\n#---- Export Data ----#\n# NOTE: Copy and paste the code below for your needs\n\n# Flatten the matrix in C-Contiguous manner\nZ = Z.flatten()\n\n## Choose floats or doubles ##\n# Pack as FLOATs:\ndata = struct.pack('>'+str(len(Z))+'f',*Z)\n# Or pack as DOUBLEs:\n#   (be sure to enable the 'Double Values' checkbox\n#   on the reader when loading into ParaView)\n#data = struct.pack('>'+str(len(Z))+'d',*Z)\n\n# Open file to write binary data\nf = open('~/test_file.bin', 'wb')\n# Write out the packed data in binary format\nf.write(data)\n# Close the file\nf.close()\n\n\n\n\nNow select \u2018File->Open\u2026\u2019 within ParaView and choose \ntest_file.bin\n wherever you saved it. This reader will unpack the floats and load them into a table. You can now use this data in many ways. For example, apply a \u2018Table to Uniform Grid\u2019 filter (from this repository) and set the filter parameters to the information in the print out from when you ran the script above. Once you have vtkImageData made of the data, you can apply a \u2018Warp by Scalar\u2019 filter for a fun visualization effect to see 3D ripples like the image below!",
            "title": "Read Binary Packed Data"
        },
        {
            "location": "/Plugins/Readers/Read-Binary-Packed-Data-to-Table/#about-this-reader",
            "text": "This filter reads in float or double data that is packed into a binary file format. It will treat the data as one long array and make a vtkTable with one column of that data. The reader uses big endian and defaults to import as floats. Use the \u2018Table to Uniform Grid\u2019 or the \u2018Reshape Table\u2019 filters to give more meaning to the data. We chose to use a vtkTable object as the output of this reader because it gives us more flexibility in the filters we can apply to this data down the pipeline and keeps thing simple when using filters in this repository.  There is a checkbox you can use to specify double precision if needed and a text box where you can specify the data variable name. The default for the data variable name is the base name of the file, however you may want something less verbose or more specific.",
            "title": "About this Reader"
        },
        {
            "location": "/Plugins/Readers/Read-Binary-Packed-Data-to-Table/#down-the-pipeline",
            "text": "Table to Uniform Grid  Reshape Table  Table to Points  Table to Structured Grid  Normalize Array",
            "title": "Down the Pipeline"
        },
        {
            "location": "/Plugins/Readers/Read-Binary-Packed-Data-to-Table/#example-use",
            "text": "For an example of how to use this reader, lets make our own test file of packed floats in binary format. Run this script outside of ParaView to create a test file. It will compute a ripple function and write out all the  Z  data as packed floats.  import struct\nimport numpy as np\n\n# The 2D space we are working with\nstep = np.pi/64.\nx = y = np.arange(-np.pi,np.pi + step, step)\nX, Y = np.meshgrid(x,y)\n\n# A cool looking ripple function\ndef f(x,y):\n    return np.sin((x**2 + y**2))\n\n# Compute the values of the ripple function in our space\nZ = np.zeros(np.shape(X), dtype=np.float32)\nfor i in range(len(x)):\n    for j in range(len(y)):\n        Z[i,j] = f(X[i,j],Y[i,j])\n\n# Parameters for a 'Table to Uniform Grid' filter\nprint(\"Shape of grid (n1,n2,n3): \", np.shape(Z))\nprint(\"Spacing for all axii: \", step)\nprint(\"Origin (x,y,z): \", (np.min(x),np.min(y),0.0))\n\n\n#---- Export Data ----#\n# NOTE: Copy and paste the code below for your needs\n\n# Flatten the matrix in C-Contiguous manner\nZ = Z.flatten()\n\n## Choose floats or doubles ##\n# Pack as FLOATs:\ndata = struct.pack('>'+str(len(Z))+'f',*Z)\n# Or pack as DOUBLEs:\n#   (be sure to enable the 'Double Values' checkbox\n#   on the reader when loading into ParaView)\n#data = struct.pack('>'+str(len(Z))+'d',*Z)\n\n# Open file to write binary data\nf = open('~/test_file.bin', 'wb')\n# Write out the packed data in binary format\nf.write(data)\n# Close the file\nf.close()  Now select \u2018File->Open\u2026\u2019 within ParaView and choose  test_file.bin  wherever you saved it. This reader will unpack the floats and load them into a table. You can now use this data in many ways. For example, apply a \u2018Table to Uniform Grid\u2019 filter (from this repository) and set the filter parameters to the information in the print out from when you ran the script above. Once you have vtkImageData made of the data, you can apply a \u2018Warp by Scalar\u2019 filter for a fun visualization effect to see 3D ripples like the image below!",
            "title": "Example Use"
        },
        {
            "location": "/Plugins/Readers/Read-Delimited-File-to-Table/",
            "text": "About this Reader\n\n\nThis reader will take in any delimited text file and make a vtkTable from it. This is not much different than the default .txt or .csv reader in ParaView, however it gives us room to use our own extensions and a little more flexibility in the structure of the files we import.\n\n\nThe main advantage of having this reader and using it over the default delimited text reader is that we can specify to use tab delimiters and that we deliver to the users of this repo a delimited text reader that they manipulate to work for their file format needs.\n\n\nDown the Pipeline\n\n\n\n\nTable to Uniform Grid\n\n\nReshape Table\n\n\nTable to Points\n\n\nTable to Structured Grid\n\n\nNormalize Array",
            "title": "Read Delimited File"
        },
        {
            "location": "/Plugins/Readers/Read-Delimited-File-to-Table/#about-this-reader",
            "text": "This reader will take in any delimited text file and make a vtkTable from it. This is not much different than the default .txt or .csv reader in ParaView, however it gives us room to use our own extensions and a little more flexibility in the structure of the files we import.  The main advantage of having this reader and using it over the default delimited text reader is that we can specify to use tab delimiters and that we deliver to the users of this repo a delimited text reader that they manipulate to work for their file format needs.",
            "title": "About this Reader"
        },
        {
            "location": "/Plugins/Readers/Read-Delimited-File-to-Table/#down-the-pipeline",
            "text": "Table to Uniform Grid  Reshape Table  Table to Points  Table to Structured Grid  Normalize Array",
            "title": "Down the Pipeline"
        },
        {
            "location": "/Plugins/Filters/Table-to-Uniform-Grid/",
            "text": "About this Filter\n\n\nThis filter takes a vtkTable object with columns that represent data to be translated (reshaped) into a 3D grid (2D also works, just set the third dimensions extent to 1). The grid will be a n1 by n2 by n3 vtkImageData structure and an origin (south-west bottom corner) can be set at any xyz point. Each column of the vtkTable will represent a data attribute of the vtkImageData formed (essentially a uniform mesh). The SEPlib option allows you to unfold data that was packed in the SEPlib format where the most important dimension is z and thus the z data is d1 (d1=z, d2=x, d3=y). When using SEPlib, specify n1 as the number of elements in the Z-direction, n2 as the number of elements in the X-direction, and n3 as the number of elements in the Y-direction (and so on for other parameters).\n\n\nParameters\n\n\n\n\nExtent: The number of elements in each of the three axial directions (element 0 (n1) corresponds to x, element 1 (n2) corresponds to y, and element 2 (n3) corresponds to z)\n\n\nUse SEPlib: a boolean for if you want to use the SEPlib axial conventions that n1 corresponds to z, n2 corresponds to x, and n3 corresponds to y.\n\n\nSpacing: the spacing along each axial direction. Usually we specify a consistent spacing across all axial directions, but you can specify to have unique spacings along each axial direction.\n\n\nOrigin: the d1, d2, and d3 (x,y,z) coordinates for the south-west bottom corner of the data set. This is the corner from which we build the volume out. Note that you can translate this specification using the \nTranslate Origin of Grid\n filter.\n\n\n\n\nDown the Pipeline\n\n\n\n\nTranslate Origin of Grid\n\n\nFlip Grid Axii\n\n\nNormalize Array\n\n\nContour\n\n\nThreshold\n\n\n\n\nExample Use\n\n\nSay we have some data in 1D format or a series of 1D data sets, like a vtkTable where we have columns of data which we know can be restructured into a 2D or 3D volume. One great example is the the table made in the example for using the \nRead Binary Packed Data\n reader. Follow the instructions to read in that data to a vtkTable. Once you have sample data in a vtkTable, we can apply a the \u2018Table to Uniform Grid\u2019 Filter and specify the shape of our volumetric data (for 2D data like this example, specify n1 and n2 accordingly and leave n3 as 1). The script provided in the example will output the extent, origin, and spacing parameters for you to use (best to copy/paste from that output into the parameter fields). This example will produce the 2D grid depicted on the \nRead Binary Packed Data\n page (that image adds the \u2018Warp by Scalar\u2019 filter).\n\n\nAnother example is to use one of the data files from \nthis website\n and load it in using the \nGSLIB File to Table\n reader. These files are in the SGeMS file format but can also be read by the GSLIB file reader. Through loading this data into a table and then applying a Table to Uniform Grid Filter, we are effectively mimicking what the \nSGeMS Grid\n reader is doing behind the scenes. These SGeMS files make great example because they outline how we can transfer any data with any number of data arrays to a uniform grid (each data array in the input table will represent a different attribute of the space made up by the vtkImageData grid). The GSLIB reader will print out the dimensions of the grid to the Output Messages console (to see this, select View->Output Messages). Use those dimensions for the n1, n2, and n3 parameters. Play around with the other parameters to get a feel for how this filter behaves.",
            "title": "Table to Uniform Grid"
        },
        {
            "location": "/Plugins/Filters/Table-to-Uniform-Grid/#about-this-filter",
            "text": "This filter takes a vtkTable object with columns that represent data to be translated (reshaped) into a 3D grid (2D also works, just set the third dimensions extent to 1). The grid will be a n1 by n2 by n3 vtkImageData structure and an origin (south-west bottom corner) can be set at any xyz point. Each column of the vtkTable will represent a data attribute of the vtkImageData formed (essentially a uniform mesh). The SEPlib option allows you to unfold data that was packed in the SEPlib format where the most important dimension is z and thus the z data is d1 (d1=z, d2=x, d3=y). When using SEPlib, specify n1 as the number of elements in the Z-direction, n2 as the number of elements in the X-direction, and n3 as the number of elements in the Y-direction (and so on for other parameters).",
            "title": "About this Filter"
        },
        {
            "location": "/Plugins/Filters/Table-to-Uniform-Grid/#parameters",
            "text": "Extent: The number of elements in each of the three axial directions (element 0 (n1) corresponds to x, element 1 (n2) corresponds to y, and element 2 (n3) corresponds to z)  Use SEPlib: a boolean for if you want to use the SEPlib axial conventions that n1 corresponds to z, n2 corresponds to x, and n3 corresponds to y.  Spacing: the spacing along each axial direction. Usually we specify a consistent spacing across all axial directions, but you can specify to have unique spacings along each axial direction.  Origin: the d1, d2, and d3 (x,y,z) coordinates for the south-west bottom corner of the data set. This is the corner from which we build the volume out. Note that you can translate this specification using the  Translate Origin of Grid  filter.",
            "title": "Parameters"
        },
        {
            "location": "/Plugins/Filters/Table-to-Uniform-Grid/#down-the-pipeline",
            "text": "Translate Origin of Grid  Flip Grid Axii  Normalize Array  Contour  Threshold",
            "title": "Down the Pipeline"
        },
        {
            "location": "/Plugins/Filters/Table-to-Uniform-Grid/#example-use",
            "text": "Say we have some data in 1D format or a series of 1D data sets, like a vtkTable where we have columns of data which we know can be restructured into a 2D or 3D volume. One great example is the the table made in the example for using the  Read Binary Packed Data  reader. Follow the instructions to read in that data to a vtkTable. Once you have sample data in a vtkTable, we can apply a the \u2018Table to Uniform Grid\u2019 Filter and specify the shape of our volumetric data (for 2D data like this example, specify n1 and n2 accordingly and leave n3 as 1). The script provided in the example will output the extent, origin, and spacing parameters for you to use (best to copy/paste from that output into the parameter fields). This example will produce the 2D grid depicted on the  Read Binary Packed Data  page (that image adds the \u2018Warp by Scalar\u2019 filter).  Another example is to use one of the data files from  this website  and load it in using the  GSLIB File to Table  reader. These files are in the SGeMS file format but can also be read by the GSLIB file reader. Through loading this data into a table and then applying a Table to Uniform Grid Filter, we are effectively mimicking what the  SGeMS Grid  reader is doing behind the scenes. These SGeMS files make great example because they outline how we can transfer any data with any number of data arrays to a uniform grid (each data array in the input table will represent a different attribute of the space made up by the vtkImageData grid). The GSLIB reader will print out the dimensions of the grid to the Output Messages console (to see this, select View->Output Messages). Use those dimensions for the n1, n2, and n3 parameters. Play around with the other parameters to get a feel for how this filter behaves.",
            "title": "Example Use"
        },
        {
            "location": "/Plugins/Filters/Flip-Grid-Axii/",
            "text": "More to come!\n\n\n\n\n\nThis filter will flip ImageData on any of the three cartesian axii. A checkbox is provided for each axis on which you may desire to flip the data.",
            "title": "Flip Grid Axii"
        },
        {
            "location": "/Plugins/Filters/Reshape-Table/",
            "text": "More to come!\n\n\n\n\n\nThis filter will take a vtkTable object and reshape it. This filter essentially treats vtkTables as 2D matrices and reshapes them using numpy.reshape in a C contiguous manner. Unfortunately, data fields will be renamed arbitrarily because VTK data arrays require a name.",
            "title": "Reshape Table"
        },
        {
            "location": "/Plugins/Filters/Add-Cell-Connectivity-to-Points/",
            "text": "More to come!\n\n\n\n\n\nThis filter will add linear cell connectivity between scattered points. You have the option to add VTK_Line or VTK_PolyLine connectivity. VTK_Line connectivity makes a straight line between the points in order (either in the order by index or using a nearest neighbor calculation). The VTK_PolyLine adds a poly line connectivity between all points as one spline (either in the order by index or using a nearest neighbor calculation).",
            "title": "Add Cell Connectivity to Points"
        },
        {
            "location": "/Plugins/Filters/Points-to-Tube/",
            "text": "More to come!\n\n\n\n\n\nTakes points from a vtkPolyData object and constructs a line of those points then builds a polygonal tube around that line with some specified radius and number of sides.",
            "title": "Points to Tube"
        },
        {
            "location": "/Plugins/Filters/Translate-Origin-of-Grid/",
            "text": "More to come!\n\n\n\n\n\nThis filter will translate the origin of vtkImageData to any specified Corner of the data set assuming it is currently in the South West Bottom Corner (will not work if Corner was moved prior).",
            "title": "Translate Origin of Grid"
        },
        {
            "location": "/Plugins/Filters/Normalize-Array/",
            "text": "More to come!\n\n\n\n\n\nThis filter allow the user to select an array from the input data set to be normalized. The filter will append another array to that data set for the output. The user can specify how they want to rename the array, can choose a multiplier, and can choose from two types of common normalizations: Feature Scaling and Standard Score.",
            "title": "Normalize Array"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/",
            "text": "PVGPpy\n\n\nPVGPpy is a python module we are developing for direct use of our macros in the ParaView shell. This module will contain the bulk of our macros for your use. We are publishing our macros in this manner to:\n\n\n\n\nStreamline their use by allowing users to call the macros like python functions directly from the ParaView shell.\n\n\nEasily update/change the macros without constant merge conflictions as users will need to input certain parameters for their use. This is much easily done through function calls than overwriting the macro files.\n\n\n\n\n\n\nMacros vs. Scripts\n\n\n\nWe will from now on refer to macros as a set of common codes that can be used regardless of data sets or scenes in ParaView. ParaView\u2019s sense of macro is not robust enough for us, so we will be referring to traditional macros in ParaView as \u2018scripts\u2019 from here on. Scripts will be used on specific sets of data where as macros can be used on any set of data.\n\n\nMacros\n\n\nMacros are Python codes that complete tedious or recurring tasks either in ParaView\u2019s gui or in ParaView\u2019s batch processing environment. We will use macros to complete common tasks like saving screenshots of isometric views of a data scene or tedious tasks like making numerous slices of a single data set along a line.\n\n\nScripts\n\n\nScripts are Python codes we will use for tasks like loading scenes and for applying several macros at once. It is often helpful to set up a script for a project so that you can easily run all the visualization processing at once each time you update your model files or create new versions of your data.\n\n\n\n\nHow to Run Scripts\n\n\nUse the Python Shell from \u2018Tools->Python Shell\u2019 in the ParaView GUI. Do not import scripts as macros in ParaView as they become static in the ParaView GUI and make managing/changing quite difficult. To use scripts in the batch processing environment, use the \npvpython\n program delivered in ParaView. On my OS X operating system it is under the \nApplications/ParaView/Contents/bin/pvpython\n. More info on all of this to come! \n\n\nTo simply run the scripts in this repo, edit the script files under the \nscripts/\n directory for your use, then run them in ParaView by selecting \u2018Tools->Python Shell\u2019 then click \u2018Run Script\u2019. Navigate to the \nscripts/\n directory in this repo and select the script you desire to use.\n\n\n\n\nMake Your Own Scripts\n\n\nDescription to come! There are a lot of pages in the documentation and we are trying to fill all content as soon as possible. Stay tuned for updates to this page\n\n\n\nUsing the Trace Tool\n\n\n\nDescription to come!\n\n\nUsing PVPython\n\n\n\nDescription to come!",
            "title": "About PVGPpy"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#pvgppy",
            "text": "PVGPpy is a python module we are developing for direct use of our macros in the ParaView shell. This module will contain the bulk of our macros for your use. We are publishing our macros in this manner to:   Streamline their use by allowing users to call the macros like python functions directly from the ParaView shell.  Easily update/change the macros without constant merge conflictions as users will need to input certain parameters for their use. This is much easily done through function calls than overwriting the macro files.",
            "title": "PVGPpy"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#macros-vs-scripts",
            "text": "We will from now on refer to macros as a set of common codes that can be used regardless of data sets or scenes in ParaView. ParaView\u2019s sense of macro is not robust enough for us, so we will be referring to traditional macros in ParaView as \u2018scripts\u2019 from here on. Scripts will be used on specific sets of data where as macros can be used on any set of data.",
            "title": "Macros vs. Scripts"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#macros",
            "text": "Macros are Python codes that complete tedious or recurring tasks either in ParaView\u2019s gui or in ParaView\u2019s batch processing environment. We will use macros to complete common tasks like saving screenshots of isometric views of a data scene or tedious tasks like making numerous slices of a single data set along a line.",
            "title": "Macros"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#scripts",
            "text": "Scripts are Python codes we will use for tasks like loading scenes and for applying several macros at once. It is often helpful to set up a script for a project so that you can easily run all the visualization processing at once each time you update your model files or create new versions of your data.",
            "title": "Scripts"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#how-to-run-scripts",
            "text": "Use the Python Shell from \u2018Tools->Python Shell\u2019 in the ParaView GUI. Do not import scripts as macros in ParaView as they become static in the ParaView GUI and make managing/changing quite difficult. To use scripts in the batch processing environment, use the  pvpython  program delivered in ParaView. On my OS X operating system it is under the  Applications/ParaView/Contents/bin/pvpython . More info on all of this to come!   To simply run the scripts in this repo, edit the script files under the  scripts/  directory for your use, then run them in ParaView by selecting \u2018Tools->Python Shell\u2019 then click \u2018Run Script\u2019. Navigate to the  scripts/  directory in this repo and select the script you desire to use.",
            "title": "How to Run Scripts"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#make-your-own-scripts",
            "text": "Description to come! There are a lot of pages in the documentation and we are trying to fill all content as soon as possible. Stay tuned for updates to this page",
            "title": "Make Your Own Scripts"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#using-the-trace-tool",
            "text": "Description to come!",
            "title": "Using the Trace Tool"
        },
        {
            "location": "/PVGPpy/About-PVGPpy/#using-pvpython",
            "text": "Description to come!",
            "title": "Using PVPython"
        },
        {
            "location": "/Scripts/Example-Script/",
            "text": "Description to come! There are a lot of pages in the documentation and we are trying to fill all content as soon as possible. Stay tuned for updates to this page",
            "title": "Example Script"
        },
        {
            "location": "/PVGPpy/export/exportVTKjs/",
            "text": "PVGPpy.export.exportVTKjs\n\n\nPVGPpy.export.exportVTKjs(FileName='', compress=False)\n\n\n\n\nDescription\n\n\nThis function will execute a script to export the current scene from your rendering into the VTKjs shareable file format.\n\n\nParameters\n\n\nFileName\n : string, optional\n\n\n\n\nUse to specify the basename of the output file. Extension will always be \u2018.vtkjs\u2019\n\n\n\n\ncompress\n : boolean, optional\n\n\n\n\nDeclares a preference to compress the data arrays.\n\n\nDefault False\n\n\n\n\nReturns\n\n\n\n\nNo return type, but it will print the path to the saved \u2018.vtkjs\u2019 file.\n\n\n\n\nNotes\n\n\n\n\nTo view, open the file in the VTKjs standalone web viewer found here: https://kitware.github.io/vtk-js/examples/StandaloneSceneLoader/StandaloneSceneLoader.html\n\n\nUse the get_vtkjs_url.py script in the PVGP repository to get a shareable link for the exported file.",
            "title": "Export as VTKjs"
        },
        {
            "location": "/PVGPpy/export/exportVTKjs/#pvgppyexportexportvtkjs",
            "text": "PVGPpy.export.exportVTKjs(FileName='', compress=False)",
            "title": "PVGPpy.export.exportVTKjs"
        },
        {
            "location": "/PVGPpy/export/exportVTKjs/#description",
            "text": "This function will execute a script to export the current scene from your rendering into the VTKjs shareable file format.",
            "title": "Description"
        },
        {
            "location": "/PVGPpy/export/exportVTKjs/#parameters",
            "text": "FileName  : string, optional   Use to specify the basename of the output file. Extension will always be \u2018.vtkjs\u2019   compress  : boolean, optional   Declares a preference to compress the data arrays.  Default False",
            "title": "Parameters"
        },
        {
            "location": "/PVGPpy/export/exportVTKjs/#returns",
            "text": "No return type, but it will print the path to the saved \u2018.vtkjs\u2019 file.",
            "title": "Returns"
        },
        {
            "location": "/PVGPpy/export/exportVTKjs/#notes",
            "text": "To view, open the file in the VTKjs standalone web viewer found here: https://kitware.github.io/vtk-js/examples/StandaloneSceneLoader/StandaloneSceneLoader.html  Use the get_vtkjs_url.py script in the PVGP repository to get a shareable link for the exported file.",
            "title": "Notes"
        },
        {
            "location": "/PVGPpy/vis/Clip-Through/",
            "text": "PVGPpy.vis.clipThrough\n\n\nPVGPpy.vis.clipThrough(clip, ax, bounds, num=10, delay=1.0)\n\n\n\n\nDescription\n\n\nThis macro takes a clip source and progresses its location through a set of bounds in the data scene. The macro requires that the clip already exist in the pipeline. This is especially useful if you have many clips linked together as all will move through the seen as a result of this macro.\n\n\nParameters\n\n\nclip\n : string\n\n\n\n\nThe string name of the clip source to be translated.\n\n\n\n\nax\n : int\n\n\n\n\nThis is the axis on which to translate (0 for x, 1 for y, 2 for z).\n\n\nThink of this as the normal vector for the clip.\n\n\n\n\nbounds\n : 6-element list or tuple\n\n\n\n\nThese are the bounds to constrain the clip translation.\n\n\n\n\nnum\n : int, optional\n\n\n\n\nThe number of discritizations in the clip translation.\n\n\n\n\ndelay\n : float, optional\n\n\n\n\nTime delay in seconds before conducting each clip translation.",
            "title": "Clip Through"
        },
        {
            "location": "/PVGPpy/vis/Clip-Through/#pvgppyvisclipthrough",
            "text": "PVGPpy.vis.clipThrough(clip, ax, bounds, num=10, delay=1.0)",
            "title": "PVGPpy.vis.clipThrough"
        },
        {
            "location": "/PVGPpy/vis/Clip-Through/#description",
            "text": "This macro takes a clip source and progresses its location through a set of bounds in the data scene. The macro requires that the clip already exist in the pipeline. This is especially useful if you have many clips linked together as all will move through the seen as a result of this macro.",
            "title": "Description"
        },
        {
            "location": "/PVGPpy/vis/Clip-Through/#parameters",
            "text": "clip  : string   The string name of the clip source to be translated.   ax  : int   This is the axis on which to translate (0 for x, 1 for y, 2 for z).  Think of this as the normal vector for the clip.   bounds  : 6-element list or tuple   These are the bounds to constrain the clip translation.   num  : int, optional   The number of discritizations in the clip translation.   delay  : float, optional   Time delay in seconds before conducting each clip translation.",
            "title": "Parameters"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/",
            "text": "PVGPpy.vis.manySlicesAlongPoints\n\n\nPVGPpy.vis.manySlicesAlongPoints(pointsNm, dataNm, numSlices=10, exportpath='', ext='.csv')\n\n\n\n\nDescription\n\n\nThis macro takes a series of points and a data source to be sliced. The points are used to construct a path through the data source and a slice is added at intervals of that path along the vector of that path at that point. This constructs \nnumSlices\n slices through the dataset \ndataNm\n.\n\n\nParameters\n\n\npointsNm\n : string\n\n\n\n\nThe string name of the points source to construct the path.\n\n\n\n\ndataNm\n : string\n\n\n\n\nThe string name of the data source to slice.\n\n\nMake sure this data source is slice-able.\n\n\n\n\nnumSlices\n : int, optional\n\n\n\n\nThe number of slices along the path.\n\n\n\n\nexportpath\n : string, optional\n\n\n\n\nThe absolute file path of where to save each slice\n\n\n\n\next\n : string, optional\n\n\n\n\nThe file extension for saving out the slices.\n\n\nDefault to \u2018.csv\u2019\n\n\n\n\nNotes\n\n\n\n\nMake sure the input data source is slice-able.\n\n\nThe SciPy module is required for this macro.\n\n\n\n\n\n\nMotivation\n\n\nSometimes we might have a model, some input data, that we would like to have numerous slices of along a series of points, a path of points per say. These points might represent some travel path through the model where we would like to have a slice of the model at each point so that we can make spacial decisions and share this information of flat documents.\n\n\nGoal\n\n\nCreate a macro that uses a series of points to create a path through a dataset that can then be sliced at many points (or customized to select ten or twenty points to slice on). The points will be converted into a sorted polyline using a nearest neighbor approximation so that we can have a coherent travel path through the model. The order in the poly line will be used to determine a normal vector for each slice.\n\n\nExample Use\n\n\nNote: You will need the SciPy module in \npvpython\n for this macro to work. \nSee details\n.\n\n\nSet the inputs\n\n\nThis macro takes two data sources, some data containing the points for our travel path and some data that can be sliced. The function calls for the string name of both of those datasets as specified above.\n\n\nThe \npointsNm\n variable will be the string name of the data source that has the point data you would like to use. Think of these points as a travel path, as we will perform a nearest neighbor route between all of the points to create a polyline. A scattered point dataset will not work well for the macro as the slices will have seemingly random orientations.\n\n\nThe \ndataNm\n variable will be the string name of the data source or model of which you desire to have interior slices. It is important that this data source is \u2018slice-able,\u2019 as some data types in VTK may not be sliceable (such as a point set). If errors arise, make sure this data set is slice-able by applying a simple slice filter from the Filters->Common->Slice.\n\n\n\n\n\nIf you have a series of points and a data set, go ahead and run this macro in the Python Shell (Tools->Python Shell):\n\n\n>>> from PVGPpy import *\n>>> macros.manySlicesAlongPoints('Points', 'Data')\n\n\n\n\nSaving\n\n\nIf you desire to save out the slices, you just made with this macro, then set the \nexportpath\n optional variable when calling the method. Be sure to give that directory a meaningful name and use that directory only for these slices as the slices will be saved out as \u2018slice0.csv\u2019, \u2018slice1.csv\u2019, and so on.\n\n\nBatch Processing\n\n\n\nIf you want to make tons of slices of a model, the outputs of this macro WILL get messy if used in the ParaView GUI. We recommend using the \npvpython\n module on the command line to perform large batch processing like this. More details to come\u2026 stay tuned.",
            "title": "Many Slices Along Points"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#pvgppyvismanyslicesalongpoints",
            "text": "PVGPpy.vis.manySlicesAlongPoints(pointsNm, dataNm, numSlices=10, exportpath='', ext='.csv')",
            "title": "PVGPpy.vis.manySlicesAlongPoints"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#description",
            "text": "This macro takes a series of points and a data source to be sliced. The points are used to construct a path through the data source and a slice is added at intervals of that path along the vector of that path at that point. This constructs  numSlices  slices through the dataset  dataNm .",
            "title": "Description"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#parameters",
            "text": "pointsNm  : string   The string name of the points source to construct the path.   dataNm  : string   The string name of the data source to slice.  Make sure this data source is slice-able.   numSlices  : int, optional   The number of slices along the path.   exportpath  : string, optional   The absolute file path of where to save each slice   ext  : string, optional   The file extension for saving out the slices.  Default to \u2018.csv\u2019",
            "title": "Parameters"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#notes",
            "text": "Make sure the input data source is slice-able.  The SciPy module is required for this macro.",
            "title": "Notes"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#motivation",
            "text": "Sometimes we might have a model, some input data, that we would like to have numerous slices of along a series of points, a path of points per say. These points might represent some travel path through the model where we would like to have a slice of the model at each point so that we can make spacial decisions and share this information of flat documents.",
            "title": "Motivation"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#goal",
            "text": "Create a macro that uses a series of points to create a path through a dataset that can then be sliced at many points (or customized to select ten or twenty points to slice on). The points will be converted into a sorted polyline using a nearest neighbor approximation so that we can have a coherent travel path through the model. The order in the poly line will be used to determine a normal vector for each slice.",
            "title": "Goal"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#example-use",
            "text": "Note: You will need the SciPy module in  pvpython  for this macro to work.  See details .",
            "title": "Example Use"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#set-the-inputs",
            "text": "This macro takes two data sources, some data containing the points for our travel path and some data that can be sliced. The function calls for the string name of both of those datasets as specified above.  The  pointsNm  variable will be the string name of the data source that has the point data you would like to use. Think of these points as a travel path, as we will perform a nearest neighbor route between all of the points to create a polyline. A scattered point dataset will not work well for the macro as the slices will have seemingly random orientations.  The  dataNm  variable will be the string name of the data source or model of which you desire to have interior slices. It is important that this data source is \u2018slice-able,\u2019 as some data types in VTK may not be sliceable (such as a point set). If errors arise, make sure this data set is slice-able by applying a simple slice filter from the Filters->Common->Slice.   If you have a series of points and a data set, go ahead and run this macro in the Python Shell (Tools->Python Shell):  >>> from PVGPpy import *\n>>> macros.manySlicesAlongPoints('Points', 'Data')",
            "title": "Set the inputs"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#saving",
            "text": "If you desire to save out the slices, you just made with this macro, then set the  exportpath  optional variable when calling the method. Be sure to give that directory a meaningful name and use that directory only for these slices as the slices will be saved out as \u2018slice0.csv\u2019, \u2018slice1.csv\u2019, and so on.",
            "title": "Saving"
        },
        {
            "location": "/PVGPpy/vis/Many-Slices-Along-Points/#batch-processing",
            "text": "If you want to make tons of slices of a model, the outputs of this macro WILL get messy if used in the ParaView GUI. We recommend using the  pvpython  module on the command line to perform large batch processing like this. More details to come\u2026 stay tuned.",
            "title": "Batch Processing"
        },
        {
            "location": "/Examples/Exporting-Visualizations/",
            "text": "Motivation\n\n\nIn order to effectively communicate our geoscientific findings, we often need to share our 3D visualizations with interested stakeholders. These interested parties are likely not going to have ParaView or other visualization software at hand. Thus we desire to have a means to export our complex visualizations in ParaView to a simple, shareable format that anyone can view. To accomplish this, we will take advantage of vtk.js and its standalone web viewer for vtk.js formats.\n\n\nWould not it be great to send your client or interested parties an interactive 3D scene of your Geophysical findings like the example below?\n\n\n\n        \n\n\n\n\n\n\n\nVTK.js\n\n\nvtk.js\n is a rendering library made for scientific visualization on the web. This code base brings high performance rendering into anyone\u2019s web browser. This library allows us to export complex scenes from ParaView and share them with anyone that has a web browser like Safari or Google Chrome.\n\n\nThe vtk.js library has an open-source \nstandalone scene viewer\n which they have a nice demo for \nhere\n. The first link can either be downloaded as an HTML file to be ran locally, or you can go to that link and run from the vtk.js server. vtk.js also published a scene export macro for ParaView that compresses a data scene in ParaView to a shareable format for viewing on the web. The macro from the vtk.js library can be found \nhere\n but we also deploy an updated (we think more robust) version of this export macro in the sub-module \nexport\n of our Python module \nPVGPpy\n.\n\n\nTest It Out\n\n\nHere are some samples to demonstrate the web viewer. We have included a few of our scenes and one of the vtk.js sample scenes for you to demo:\n\n\n\n\nVolcano\n\n\nVolcano Clipped\n\n\nRipple\n\n\nTunnels\n\n\nvtk.js Sample Scene\n\n\n\n\n\n\nMake Your Own\n\n\nFirst, make a complex scene in ParaView that you might like to share with someone. \n Now that you have your scene loaded, open the python shell from Tools->Python Shell within ParaView. From here, import our Python module delivered in the repository called \nPVGPpy\n. From the \nexport\n sub-module, there is a function called \nexportVTKjs()\n which takes two optional arguments (\nFileName\n string and \ncompress\n boolean). Execute this function and note the output text as it will describe where the exported scene was saved.\n\n\n# Import our Python module:\nfrom PVGPpy import *\n\n# Now run the exportVTKjs script from the export sub-module\nexport.exportVTKjs(FileName='test_export')\n\n\n\n\nIf you have trouble post on our \nissues page\n or read the vtk.js documentation \nhere\n\n\nNow open the standalone web viewer by opening \nthis link\n.\n\n\nSelect the exported scene as the input file for the web viewer from where you saved it (should be under \n~/Dropbox/PVGP_vtkjs/\n). The export macro should have printed out the location of the saved scene in the Python Shell.\n\n\n\n\nHow to Share\n\n\nQuick and Easy\n\n\nTo share these exported scenes with non-technical stakeholders, we recommend the following process:\n\n\n\n\nCreate your scene and export to the vtk.js format (follow process above)\n\n\nQuality control your visualization by viewing in web browser yourself (follow process above)\n\n\nSend an email with your visualization (\n.vtkjs\n file) and something along the lines of:\n\n\n\n\nCheck out the data scene/model by downloading the attached file. Then go to the link below and open that downloaded file.\n\nhttps://kitware.github.io/vtk-js/examples/StandaloneSceneLoader/StandaloneSceneLoader.html\n\n\n\n\nA Bit More Robust\n\n\nSometimes we might want to give someone a direct link to the web visualization so all they have to do is open the link on any device and they can see our visualization. Here is a method to share scenes that have a slightly easier process of viewing the file for the end user and will handle the case for mobile platforms.\n\n\nUnfortunately, making the experience for the end user simple means making your experience a bit more complicated. You will need to host your file on a web service like GitHub or Dropbox \n(we have been unsuccessful in getting Google Drive to work)\n. Then get a public link to the \n.vtkjs\n file on that web service and append it to the web viewer URL.\n\n\nWe have created a Python script to generate these links for you if you are sharing your data file on either Dropbox or GitHub. The script is delivered in the repository and can also be found \nhere\n.\n\n\nThe easiest way that we have found is to share the files on Dropbox. Use the desktop client for Dropbox and right-click your exported \n.vtkjs\n file and select \u201cCopy Dropbox Link.\u201d\n\n\nOnce you have that link, use the this script on your URLs in this manner:\n\n\n# Usage:\n$ python get_vtkjs_url.py <web file host> <file link>\n\n# Dropbox example:\n$ python get_vtkjs_url.py dropbox \"https://www.dropbox.com/s/6m5ttdbv5bf4ngj/ripple.vtkjs?dl=0\"",
            "title": "Exporting Visualizations"
        },
        {
            "location": "/Examples/Exporting-Visualizations/#motivation",
            "text": "In order to effectively communicate our geoscientific findings, we often need to share our 3D visualizations with interested stakeholders. These interested parties are likely not going to have ParaView or other visualization software at hand. Thus we desire to have a means to export our complex visualizations in ParaView to a simple, shareable format that anyone can view. To accomplish this, we will take advantage of vtk.js and its standalone web viewer for vtk.js formats.  Would not it be great to send your client or interested parties an interactive 3D scene of your Geophysical findings like the example below?",
            "title": "Motivation"
        },
        {
            "location": "/Examples/Exporting-Visualizations/#vtkjs",
            "text": "vtk.js  is a rendering library made for scientific visualization on the web. This code base brings high performance rendering into anyone\u2019s web browser. This library allows us to export complex scenes from ParaView and share them with anyone that has a web browser like Safari or Google Chrome.  The vtk.js library has an open-source  standalone scene viewer  which they have a nice demo for  here . The first link can either be downloaded as an HTML file to be ran locally, or you can go to that link and run from the vtk.js server. vtk.js also published a scene export macro for ParaView that compresses a data scene in ParaView to a shareable format for viewing on the web. The macro from the vtk.js library can be found  here  but we also deploy an updated (we think more robust) version of this export macro in the sub-module  export  of our Python module  PVGPpy .",
            "title": "VTK.js"
        },
        {
            "location": "/Examples/Exporting-Visualizations/#test-it-out",
            "text": "Here are some samples to demonstrate the web viewer. We have included a few of our scenes and one of the vtk.js sample scenes for you to demo:   Volcano  Volcano Clipped  Ripple  Tunnels  vtk.js Sample Scene",
            "title": "Test It Out"
        },
        {
            "location": "/Examples/Exporting-Visualizations/#make-your-own",
            "text": "First, make a complex scene in ParaView that you might like to share with someone.   Now that you have your scene loaded, open the python shell from Tools->Python Shell within ParaView. From here, import our Python module delivered in the repository called  PVGPpy . From the  export  sub-module, there is a function called  exportVTKjs()  which takes two optional arguments ( FileName  string and  compress  boolean). Execute this function and note the output text as it will describe where the exported scene was saved.  # Import our Python module:\nfrom PVGPpy import *\n\n# Now run the exportVTKjs script from the export sub-module\nexport.exportVTKjs(FileName='test_export')  If you have trouble post on our  issues page  or read the vtk.js documentation  here  Now open the standalone web viewer by opening  this link .  Select the exported scene as the input file for the web viewer from where you saved it (should be under  ~/Dropbox/PVGP_vtkjs/ ). The export macro should have printed out the location of the saved scene in the Python Shell.",
            "title": "Make Your Own"
        },
        {
            "location": "/Examples/Exporting-Visualizations/#how-to-share",
            "text": "",
            "title": "How to Share"
        },
        {
            "location": "/Examples/Exporting-Visualizations/#quick-and-easy",
            "text": "To share these exported scenes with non-technical stakeholders, we recommend the following process:   Create your scene and export to the vtk.js format (follow process above)  Quality control your visualization by viewing in web browser yourself (follow process above)  Send an email with your visualization ( .vtkjs  file) and something along the lines of:   Check out the data scene/model by downloading the attached file. Then go to the link below and open that downloaded file.\n\nhttps://kitware.github.io/vtk-js/examples/StandaloneSceneLoader/StandaloneSceneLoader.html",
            "title": "Quick and Easy"
        },
        {
            "location": "/Examples/Exporting-Visualizations/#a-bit-more-robust",
            "text": "Sometimes we might want to give someone a direct link to the web visualization so all they have to do is open the link on any device and they can see our visualization. Here is a method to share scenes that have a slightly easier process of viewing the file for the end user and will handle the case for mobile platforms.  Unfortunately, making the experience for the end user simple means making your experience a bit more complicated. You will need to host your file on a web service like GitHub or Dropbox  (we have been unsuccessful in getting Google Drive to work) . Then get a public link to the  .vtkjs  file on that web service and append it to the web viewer URL.  We have created a Python script to generate these links for you if you are sharing your data file on either Dropbox or GitHub. The script is delivered in the repository and can also be found  here .  The easiest way that we have found is to share the files on Dropbox. Use the desktop client for Dropbox and right-click your exported  .vtkjs  file and select \u201cCopy Dropbox Link.\u201d  Once you have that link, use the this script on your URLs in this manner:  # Usage:\n$ python get_vtkjs_url.py <web file host> <file link>\n\n# Dropbox example:\n$ python get_vtkjs_url.py dropbox \"https://www.dropbox.com/s/6m5ttdbv5bf4ngj/ripple.vtkjs?dl=0\"",
            "title": "A Bit More Robust"
        },
        {
            "location": "/Examples/Slice-Model-Along-PolyLine/",
            "text": "More details to come \u2026 here\u2019s a brief outline\n\n\n\n\n\nWhat if we have a series of points and we desire to create one slice of a data set along the entirety of those points? Then we need to create connectivity between those points and use the native \u2018Slice Along Poly Line\u2019 filter.\n\n\nFirst, make a data source that is point data. Maybe you have x,y,z points in a csv file, read in the file and perform a \u2018Table to Points\u2019 filter. Now add cell connectivity between the points in the form of a poly line by using the \u2018Add Cell Connectivity to Points\u2019 filter delivered in this repository.\n\n\nOnce you have points will poly line connectivity, then apply the \u2018Slice Along Poly Line\u2019 filter that is native in ParaView. Specify the connected points as the poly line source and your data to be sliced as the data input to that filter.",
            "title": "Slice Model Along PolyLine"
        },
        {
            "location": "/Virtual-Reality/Entering-Virtual-Reality/",
            "text": "Description to come! There are a lot of pages in the documentation and we are trying to fill all content as soon as possible. Stay tuned for updates to this page",
            "title": "Entering Virtual Reality"
        }
    ]
}